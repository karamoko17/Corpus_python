{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc585923163f41deba9c9e5036b372af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='data', description='Mot clé:'), IntText(value=10, description='Nb articles')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "t = widgets.Text(\n",
    "    description='Mot clé:',\n",
    "    value='data',\n",
    "    disabled=False   \n",
    ")\n",
    "n = widgets.IntText( \n",
    "    description=\"Nb articles\",\n",
    "    value=10,\n",
    "    min=2\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Valider',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Valider',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fonction de gestionnaire d'événements pour le changement de valeur des champs\n",
    "def update_form(change):\n",
    "    if len(t.value.strip()) > 0 and n.value > 0:\n",
    "        # Afficher le message lorsque tous les champs sont remplis\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    else:\n",
    "        # Afficher le formulaire tant que tous les champs ne sont pas remplis\n",
    "        clear_output(wait=True)\n",
    "        display(form)\n",
    "\n",
    "# Liaison de la fonction de gestionnaire d'événements aux changements de valeur des champs\n",
    "t.observe(update_form, names='value')\n",
    "n.observe(update_form, names='value')\n",
    "\n",
    "# Création du formulaire\n",
    "form = widgets.VBox([t, n])\n",
    "\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============== PARTIE 1 =============\n",
    "# =============== 1.1 : REDDIT ===============\n",
    "# Libraries\n",
    "import praw\n",
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import datetime\n",
    "from Classes import ArxivDocument, RedditDocument\n",
    "\n",
    "\n",
    "limit = n.value\n",
    "keyword = t.value\n",
    "# Fonction affichage hiérarchie dict\n",
    "def showDictStruct(d):\n",
    "    def recursivePrint(d, i):\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                print(\"-\"*i, k)\n",
    "                recursivePrint(d[k], i+2)\n",
    "            else:\n",
    "                print(\"-\"*i, k, \":\", d[k])\n",
    "    recursivePrint(d, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "\n",
    "def rechercher_documents():\n",
    "    \n",
    "    # Identification\n",
    "    reddit = praw.Reddit(client_id='QyizTPZdvFMfbmCtobzj1g', client_secret='TkvCDzi73SAaAge2LDOgEFHb6sF0NA', user_agent='test')\n",
    "\n",
    "    hot_posts = reddit.subreddit(keyword).hot(limit=limit)#.top(\"all\", limit=limit)#\n",
    "\n",
    "    # Récupération du texte\n",
    "    docs = []\n",
    "    docs_bruts = []\n",
    "    afficher_cles = False\n",
    "    for i, post in enumerate(hot_posts):\n",
    "        if i%10==0: print(\"Reddit:\", i, \"/\", limit)\n",
    "        if afficher_cles:  # Pour connaître les différentes variables et leur contenu\n",
    "            for k, v in post.__dict__.items():\n",
    "                pass\n",
    "                print(k, \":\", v)\n",
    "\n",
    "        if post.selftext != \"\":  # Osef des posts sans texte\n",
    "            pass\n",
    "            #print(post.selftext)\n",
    "        docs.append(post.selftext.replace(\"\\n\", \" \"))\n",
    "        docs_bruts.append((\"Reddit\", post))\n",
    "\n",
    "    #print(docs)\n",
    "\n",
    "    # =============== 1.2 : ArXiv ===============\n",
    "\n",
    "    # Paramètres\n",
    "    query_terms = [keyword]\n",
    "\n",
    "    # Requête\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{\"+\".join(query_terms)}&start=0&max_results={limit}'\n",
    "    data = urllib.request.urlopen(url)\n",
    "\n",
    "    # Format dict (OrderedDict)\n",
    "    data = xmltodict.parse(data.read().decode('utf-8'))\n",
    "\n",
    "    #showDictStruct(data)\n",
    "\n",
    "    # Ajout résumés à la liste\n",
    "    '''for i, entry in enumerate(data[\"feed\"][\"entry\"]):\n",
    "        if i%10==0: print(\"ArXiv:\", i, \"/\", limit)\n",
    "        docs.append(entry[\"summary\"].replace(\"\\n\", \"\"))\n",
    "        docs_bruts.append((\"ArXiv\", entry))\n",
    "        #showDictStruct(entry)'''\n",
    "\n",
    "    # =============== 1.3 : Exploitation ===============\n",
    "    print(f\"# docs avec doublons : {len(docs)}\")\n",
    "    docs = list(set(docs))\n",
    "    print(f\"# docs sans doublons : {len(docs)}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i}\\t# caractères : {len(doc)}\\t# mots : {len(doc.split(' '))}\\t# phrases : {len(doc.split('.'))}\")\n",
    "        if len(doc)<100:\n",
    "            docs.remove(doc)\n",
    "\n",
    "    longueChaineDeCaracteres = \" \".join(docs)\n",
    "\n",
    "    # =============== PARTIE 2 =============\n",
    "\n",
    "    # =============== 2.3 : MANIPS ===============\n",
    "\n",
    "\n",
    "    for nature, doc in docs_bruts:\n",
    "        if nature == \"ArXiv\":  # Les fichiers de ArXiv ou de Reddit sont pas formatés de la même manière à ce stade.\n",
    "            #showDictStruct(doc)\n",
    "\n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "            try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "            # doc_classe = Document(titre, authors, date, doc[\"id\"], summary)  # Création du Document\n",
    "            # Adding ArxivDocument\n",
    "            arxiv_doc = ArxivDocument(title=titre, auteur=authors, date=date, url=doc[\"id\"], texte=summary)\n",
    "\n",
    "            collection.append(arxiv_doc)  # Ajout du Document à la liste.\n",
    "\n",
    "        elif nature == \"Reddit\":\n",
    "            #print(\"\".join([f\"{k}: {v}\\n\" for k, v in doc.__dict__.items()]))\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = \"https://www.reddit.com/\"+doc.permalink\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            num_comments = int(doc.num_comments)\n",
    "            # Adding RedditDocument\n",
    "            reddit_doc = RedditDocument(title=titre, auteur=auteur, date=date, url=url, texte=texte, num_comments=num_comments)\n",
    "            # doc_classe = Document(titre, auteur, date, url, texte)\n",
    "            \n",
    "            collection.append(reddit_doc)\n",
    "\n",
    "    # Création de l'index de documents\n",
    "    id2doc = {}\n",
    "    for i, doc in enumerate(collection):\n",
    "        id2doc[i] = doc.titre\n",
    "\n",
    "    # =============== 2.4, 2.5 : CLASSE AUTEURS ===============\n",
    "    from Classes import Author\n",
    "\n",
    "    # =============== 2.6 : DICT AUTEURS ===============\n",
    "    authors = {}\n",
    "    aut2id = {}\n",
    "    num_auteurs_vus = 0\n",
    "\n",
    "    # Création de la liste+index des Auteurs\n",
    "    for doc in collection:\n",
    "        if doc.auteur not in aut2id:\n",
    "            num_auteurs_vus += 1\n",
    "            authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "            aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "        authors[aut2id[doc.auteur]].add(doc.texte)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit: 0 / 10\n",
      "# docs avec doublons : 10\n",
      "# docs sans doublons : 10\n",
      "Document 0\t# caractères : 0\t# mots : 1\t# phrases : 1\n",
      "Document 1\t# caractères : 228\t# mots : 45\t# phrases : 2\n",
      "Document 2\t# caractères : 136\t# mots : 26\t# phrases : 5\n",
      "Document 3\t# caractères : 4143\t# mots : 580\t# phrases : 41\n",
      "Document 4\t# caractères : 472\t# mots : 76\t# phrases : 3\n",
      "Document 5\t# caractères : 338\t# mots : 60\t# phrases : 4\n",
      "Document 6\t# caractères : 1231\t# mots : 234\t# phrases : 21\n",
      "Document 7\t# caractères : 772\t# mots : 114\t# phrases : 11\n",
      "Document 8\t# caractères : 124\t# mots : 31\t# phrases : 3\n"
     ]
    }
   ],
   "source": [
    "if len(keyword) > 0:\n",
    "    button.on_click(rechercher_documents()) \n",
    "else:\n",
    "    print(\"Le mot clé est requis svp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre : BIG DATA FILE how to process\tAuteur : Ellie10543\tDate : 2024/01/06\tURL : https://www.reddit.com//r/data/comments/1907pls/big_data_file_how_to_process/\tTexte : I have a 50 gb CSV with 100 000 000+ records.   Nothing can open it with the RAM. How can I querry this data? or split it? \t\n",
      "Titre : data about videogame consoles\tAuteur : wilqqqq\tDate : 2024/01/07\tURL : https://www.reddit.com//r/data/comments/190xkii/data_about_videogame_consoles/\tTexte : Hello, do you guyz know where I can find data on the number of console units sold from each year from the inception of the first console until now?? I need this for my college project. I wanted to make a bar chart race animation\t\n",
      "Titre : Database Design Best Practice\tAuteur : Haboukh\tDate : 2024/01/09\tURL : https://www.reddit.com//r/data/comments/19264hk/database_design_best_practice/\tTexte :  Hello Reddit.I  am wondering what is the best practice to store information about  projects (as an example) for an internal website built using Flask.  Storage is not an issue because there isn't that much data, and my main  concern is for the database to be modular as projects may have lots of  different features and few in common.There are two scenarios that I can think of.1) Store the data in a normal table with all information related to a project are in one linehttps://preview.redd.it/py64f9xjecbc1.jpg?width=507&format=pjpg&auto=webp&s=6422265adcfa0492b2d4453ca4de3eeabd351f3d2) Store the data as in records as in the picture below https://preview.redd.it/8sj5h4cnecbc1.jpg?width=309&format=pjpg&auto=webp&s=19db2fa29ee460ec604beb8dc7621e09d2a44a5b\t\n",
      "Titre : Drinking 2023, A Year in Review\tAuteur : GuruRoo\tDate : 2024/01/10\tURL : https://www.reddit.com//r/data/comments/192t436/drinking_2023_a_year_in_review/\tTexte : \t\n",
      "Titre : Enhancing Data Governance Solutions for Effective Management\tAuteur : Beautiful-Ad-7743\tDate : 2024/01/10\tURL : https://www.reddit.com//r/data/comments/1934n7m/enhancing_data_governance_solutions_for_effective/\tTexte :  In today's data-driven world, organizations are constantly generating and collecting vast amounts of data. With this abundance of information comes the responsibility of managing and protecting it effectively. [Data governance solutions](https://www.sganalytics.com/data-solutions/data-governance-company/) have become crucial for organizations seeking to maintain data integrity, security, and compliance. In this article, we will explore the significance of data governance and various solutions that contribute to its effective implementation.# The Importance of Data Governance:Data governance refers to the overall management of the availability, usability, integrity, and security of an organization's data. A robust data governance framework is essential for ensuring that data is treated as a valuable asset and is used responsibly. Key aspects of data governance include data quality, data security, compliance with regulations, and establishing clear data ownership and accountability.# Data Governance Solutions:1. **Data Classification and Tagging:**  * Implementing a system that classifies and tags data based on its sensitivity and criticality helps organizations prioritize their data governance efforts. This allows for the enforcement of specific policies based on the classification of data.1. **Metadata Management:**  * Metadata, or data about data, is crucial for understanding and managing the characteristics and lineage of data. Effective meta [data management solutions](https://www.sganalytics.com/data-solutions/data-management-services-company/) provide insights into data sources, transformations, and relationships, aiding in data lineage tracking and ensuring data quality.1. **Access Control and Permissions:**  * Limiting access to data based on roles and responsibilities is a fundamental aspect of data governance. Access control solutions help organizations enforce policies to ensure that only authorized personnel can access, modify, or delete sensitive data.1. **Data Auditing and Monitoring:**  * Continuous monitoring and auditing of data activities are essential for identifying and addressing potential security and compliance issues. Robust data auditing solutions enable organizations to track changes, detect anomalies, and maintain an audit trail for regulatory purposes.1. **Data Encryption:**  * Encrypting data both in transit and at rest is a critical component of data governance. Encryption solutions protect sensitive information from unauthorized access and mitigate the risks associated with data breaches.1. **Data Governance Frameworks:**  * Adopting comprehensive [data governance](https://us.sganalytics.com/data-solutions/data-governance/) frameworks provides organizations with a structured approach to managing their data. Frameworks such as DAMA (Data Management Association) and GDPR (General Data Protection Regulation) help organizations establish policies, procedures, and controls for effective data governance.1. **Master Data Management (MDM):**  * MDM solutions focus on creating and maintaining a single, accurate, and consistent version of master data across the organization. This ensures that critical business information, such as customer and product data, is standardized and reliable.1. **Data Quality Management:**  * Ensuring the accuracy and reliability of data is paramount for effective decision-making. Data quality management solutions help organizations identify and rectify data anomalies, inconsistencies, and errors.Conclusion:As organizations continue to navigate the complex landscape of data management, implementing robust data governance solutions is imperative. By addressing key aspects such as data classification, metadata management, access control, auditing, encryption, and adopting governance frameworks, organizations can enhance their ability to manage and derive value from their data assets. Investing in data governance not only safeguards against potential risks but also fosters a culture of responsibility, accountability, and compliance in the ever-evolving data ecosystem. \t\n",
      "Titre : How do I get SPSS for free that is safe?\tAuteur : endgamefond\tDate : 2024/01/08\tURL : https://www.reddit.com//r/data/comments/1918mr5/how_do_i_get_spss_for_free_that_is_safe/\tTexte :  I know seeking free software to avoid piracy or unauthorized distribution. Using SPSS without a valid license is illegal and can result in serious consequences to my computer. But I cant afford it. My university also don't provide it. what should i do.Thanks&#x200B;\t\n",
      "Titre : Inferring/Generating Data when Data not Available\tAuteur : Popular-Ad-7656\tDate : 2024/01/08\tURL : https://www.reddit.com//r/data/comments/191794n/inferringgenerating_data_when_data_not_available/\tTexte : What are they looking for when answering this interview question?:When you can’t find the data that you need, you are creative enough to infer and/or generate the data needed from other information that is available.Is it supposed to mean statistical inference for a population from a sample (confidence interval), linear regression models (relationship between A &B to produce data for C), or imputing data for missing rows/columns? Any guidance would be appreciated.\t\n",
      "Titre : Interesting Stats to separate\tAuteur : JamiesBond007\tDate : 2024/01/05\tURL : https://www.reddit.com//r/data/comments/18ys4ol/interesting_stats_to_separate/\tTexte : For context, me and my friends watch movies together and have a spreadsheet for our Rankings from 1-10. Now would be the question which would be the most interesting stats to have in this table. So far we have a Leaderboard, our personal averages, our average overall and our highest/lowest ratings.What other could we add to this list?\t\n",
      "Titre : Looking for a solution to acces my Pokémon Data\tAuteur : Aligatueur\tDate : 2024/01/10\tURL : https://www.reddit.com//r/data/comments/192slpu/looking_for_a_solution_to_acces_my_pokémon_data/\tTexte : Heyah o/Yeah the title might be dumb. But it's an issue i'm facing for months (years ?) and I can't find any correct solution, and really, it's driving me crazy.Here's the situation :* I have a stupid amount of Pokemon caught and gathered. Around 8000+ * I want something to display my collection (the data), to be able to search for it, and have like a nice GUI really done for that purpose of data displaying / searching.* I want to add / remove data easily. What would be amazing would be csv import if existing software / docker etc.I've tried a stupid amount of solution. Looked for spreadsheets, inventory tools, collection tools etc. None were able to check everything. * Google Sheets is my actual setup. But my collection is too big. Lags, bad searching functions, nothing optimized. AppSheets doesn't help either.* Koillection / Homebox for docker side. Meh.* DataCrow / CGStar for software.This is pretty much the stuff i've tried. I've thought about creating my own little thing with HTML / SQL etc but I can't find anything simple that could be stored on my server and accessed from any device easily.I'm looking for any kind of solution. But i've tried a lot of things sadly.Any help ? Thanks o/\t\n",
      "(416, 416)\n"
     ]
    }
   ],
   "source": [
    "# =============== 2.7, 2.8 : CORPUS ===============\n",
    "from Corpus import Corpus\n",
    "corpus = Corpus(\"Mon corpus\")\n",
    "\n",
    "# Construction du corpus à partir des documents\n",
    "for doc in collection:\n",
    "    corpus.add(doc,)\n",
    "corpus.show(tri=\"abc\")\n",
    "# print(corpus.construire_matrice_tf())\n",
    "\n",
    "# =============== 4.1 : TEST SINGLETON ===============\n",
    "# corpus_test = Corpus(\"Mon corpus test\")\n",
    "# print(\"Instance unique de corpus ?\", corpus is corpus_test)\n",
    "\n",
    "# print(corpus)\n",
    "\n",
    "\n",
    "# Construire le dictionnaire vocab à partir de la DataFrame freq\n",
    "vocab = {}\n",
    "freq = corpus.stats()\n",
    "\n",
    "mat_TF = corpus.construire_matrice_tf(vocab=freq)\n",
    "\n",
    "vocab = corpus.construire_vocab(freq=freq)\n",
    "\n",
    "\n",
    "mat_TFxIDF = corpus.calculer_matrice_TFxIDF(documents=vocab['Mots'])\n",
    "# print(\"=====mat===== \", mat_TFxIDF)\n",
    "\n",
    "# =============== 2.9 : SAUVEGARDE ===============\n",
    "import pickle\n",
    "\n",
    "# Ouverture d'un fichier, puis écriture avec pickle\n",
    "with open(\"corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "# Supression de la variable \"corpus\"\n",
    "del corpus\n",
    "\n",
    "# Ouverture du fichier, puis lecture avec pickle\n",
    "with open(\"corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "# La variable est réapparue\n",
    "# print(corpus)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
