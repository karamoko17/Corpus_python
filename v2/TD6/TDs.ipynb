{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150a1819bcbe4631987b557d76ba3a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='data', description='Mot clé:'), IntText(value=10, description='Nb articles')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "t = widgets.Text(\n",
    "    description='Mot clé:',\n",
    "    value='data',\n",
    "    disabled=False   \n",
    ")\n",
    "n = widgets.IntText( \n",
    "    description=\"Nb articles\",\n",
    "    value=10,\n",
    "    min=2\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Valider',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Valider',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fonction de gestionnaire d'événements pour le changement de valeur des champs\n",
    "def update_form(change):\n",
    "    if len(t.value.strip()) > 0 and n.value > 0:\n",
    "        # Afficher le message lorsque tous les champs sont remplis\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    else:\n",
    "        # Afficher le formulaire tant que tous les champs ne sont pas remplis\n",
    "        clear_output(wait=True)\n",
    "        display(form)\n",
    "\n",
    "# Liaison de la fonction de gestionnaire d'événements aux changements de valeur des champs\n",
    "t.observe(update_form, names='value')\n",
    "n.observe(update_form, names='value')\n",
    "\n",
    "# Création du formulaire\n",
    "form = widgets.VBox([t, n])\n",
    "\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit: 0 / 10\n",
      "ArXiv: 0 / 10\n",
      "# docs avec doublons : 20\n",
      "# docs sans doublons : 20\n",
      "Nombre de mots différents dans le corpus : 1112\n",
      "\n",
      "Les 10 mots les plus fréquents dans le corpus :\n",
      "          Mots  Occurrence  Document_Frequency\n",
      "154       data         240                  16\n",
      "4          the         105                  18\n",
      "12         and         104                  16\n",
      "14          of          95                  15\n",
      "66          to          78                  17\n",
      "...        ...         ...                 ...\n",
      "509   creative           1                   1\n",
      "510     enough           1                   1\n",
      "511      infer           1                   1\n",
      "513     needed           1                   1\n",
      "1111   forthis           1                   1\n",
      "\n",
      "[1112 rows x 3 columns]\n",
      "  left_context matched_text right_context\n",
      "0   ges. This      document     provides \n",
      "A Forecasting-Based DLP Approach for Data Security, par Kishu Gupta, Ashwani Kush\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "A Random Sample Partition Data Model for Big Data Analysis, par Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara, Chenghao Wei, Heping He\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "A Survey on Sampling and Profiling over Big Data (Technical Report), par Zhicheng Liu, Aoqian Zhang\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking, par Zijian Ming, Chunjie Luo, Wanling Gao, Rui Han, Qiang Yang, Lei Wang, Jianfeng Zhan\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "BIG DATA FILE how to process, par Ellie10543\n",
      "Source: Reddit\n",
      "Number of Comments: 7\n",
      "data about videogame consoles, par wilqqqq\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Data Gathering from Path Constrained Mobile Sensors Using Data MULE, par Dinesh Dash\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Data Lakes, Clouds and Commons: A Review of Platforms for Analyzing and  Sharing Genomic Data, par Robert L. Grossman\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Data Science: A Comprehensive Overview, par Longbing Cao\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Database Design Best Practice, par Haboukh\n",
      "Source: Reddit\n",
      "Number of Comments: 4\n",
      "Double Hard disk Problem, par Mormell\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Drinking 2023, A Year in Review, par GuruRoo\n",
      "Source: Reddit\n",
      "Number of Comments: 2\n",
      "Enhancing Data Governance Solutions for Effective Management, par Beautiful-Ad-7743\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "How do I get SPSS for free that is safe?, par endgamefond\n",
      "Source: Reddit\n",
      "Number of Comments: 4\n",
      "Inferring/Generating Data when Data not Available, par Popular-Ad-7656\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Looking for a solution to acces my Pokémon Data, par Aligatueur\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Modern Data Formats for Big Bioinformatics Data Analytics, par Shahzad Ahmed, M. Usman Ali, Javed Ferzund, Muhammad Atif Sarwar, Abbas Rehman, Atif Mehmood\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Question for discussion: Why do companies fail when adopting Modern tooling and practices like in the MDS (Modern Data Stack), par Data-Queen-Mayra\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Technical Report: Developing a Working Data Hub, par Vijay Gadepally, Jeremy Kepner\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "The Data Station: Combining Data, Compute, and Market Forces, par Raul Castro Fernandez, Kyle Chard, Ben Blaiszik, Sanjay Krishnan, Aaron Elmore, Ziad Obermeyer, Josh Risley, Sendhil Mullainathan, Michael Franklin, Ian Foster\n",
      "Source: Arxiv\n",
      "Co-authors: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============== PARTIE 1 =============\n",
    "# =============== 1.1 : REDDIT ===============\n",
    "# Libraries\n",
    "import praw\n",
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import datetime\n",
    "from Classes import ArxivDocument, RedditDocument\n",
    "\n",
    "\n",
    "limit = n.value\n",
    "keyword = t.value\n",
    "# Fonction affichage hiérarchie dict\n",
    "def showDictStruct(d):\n",
    "    def recursivePrint(d, i):\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                print(\"-\"*i, k)\n",
    "                recursivePrint(d[k], i+2)\n",
    "            else:\n",
    "                print(\"-\"*i, k, \":\", d[k])\n",
    "    recursivePrint(d, 1)\n",
    "\n",
    "def rechercher_documents():\n",
    "    \n",
    "    # Identification\n",
    "    reddit = praw.Reddit(client_id='QyizTPZdvFMfbmCtobzj1g', client_secret='TkvCDzi73SAaAge2LDOgEFHb6sF0NA', user_agent='test')\n",
    "\n",
    "    hot_posts = reddit.subreddit(keyword).hot(limit=limit)#.top(\"all\", limit=limit)#\n",
    "\n",
    "    # Récupération du texte\n",
    "    docs = []\n",
    "    docs_bruts = []\n",
    "    afficher_cles = False\n",
    "    for i, post in enumerate(hot_posts):\n",
    "        if i%10==0: print(\"Reddit:\", i, \"/\", limit)\n",
    "        if afficher_cles:  # Pour connaître les différentes variables et leur contenu\n",
    "            for k, v in post.__dict__.items():\n",
    "                pass\n",
    "                print(k, \":\", v)\n",
    "\n",
    "        if post.selftext != \"\":  # Osef des posts sans texte\n",
    "            pass\n",
    "            #print(post.selftext)\n",
    "        docs.append(post.selftext.replace(\"\\n\", \" \"))\n",
    "        docs_bruts.append((\"Reddit\", post))\n",
    "\n",
    "    #print(docs)\n",
    "\n",
    "    # =============== 1.2 : ArXiv ===============\n",
    "\n",
    "    # Paramètres\n",
    "    query_terms = [keyword]\n",
    "\n",
    "    # Requête\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{\"+\".join(query_terms)}&start=0&max_results={limit}'\n",
    "    data = urllib.request.urlopen(url)\n",
    "\n",
    "    # Format dict (OrderedDict)\n",
    "    data = xmltodict.parse(data.read().decode('utf-8'))\n",
    "\n",
    "    #showDictStruct(data)\n",
    "\n",
    "    # Ajout résumés à la liste\n",
    "    for i, entry in enumerate(data[\"feed\"][\"entry\"]):\n",
    "        if i%10==0: print(\"ArXiv:\", i, \"/\", limit)\n",
    "        docs.append(entry[\"summary\"].replace(\"\\n\", \"\"))\n",
    "        docs_bruts.append((\"ArXiv\", entry))\n",
    "        #showDictStruct(entry)\n",
    "\n",
    "    # =============== 1.3 : Exploitation ===============\n",
    "    print(f\"# docs avec doublons : {len(docs)}\")\n",
    "    docs = list(set(docs))\n",
    "    print(f\"# docs sans doublons : {len(docs)}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        # print(f\"Document {i}\\t# caractères : {len(doc)}\\t# mots : {len(doc.split(' '))}\\t# phrases : {len(doc.split('.'))}\")\n",
    "        if len(doc)<100:\n",
    "            docs.remove(doc)\n",
    "\n",
    "    longueChaineDeCaracteres = \" \".join(docs)\n",
    "\n",
    "    # =============== PARTIE 2 =============\n",
    "\n",
    "    # =============== 2.3 : MANIPS ===============\n",
    "\n",
    "\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    for nature, doc in docs_bruts:\n",
    "        if nature == \"ArXiv\":  # Les fichiers de ArXiv ou de Reddit sont pas formatés de la même manière à ce stade.\n",
    "            #showDictStruct(doc)\n",
    "\n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "            try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "            # doc_classe = Document(titre, authors, date, doc[\"id\"], summary)  # Création du Document\n",
    "            # Adding ArxivDocument\n",
    "            arxiv_doc = ArxivDocument(title=titre, auteur=authors, date=date, url=doc[\"id\"], texte=summary)\n",
    "\n",
    "            collection.append(arxiv_doc)  # Ajout du Document à la liste.\n",
    "\n",
    "        elif nature == \"Reddit\":\n",
    "            #print(\"\".join([f\"{k}: {v}\\n\" for k, v in doc.__dict__.items()]))\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = \"https://www.reddit.com/\"+doc.permalink\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            num_comments = int(doc.num_comments)\n",
    "            # Adding RedditDocument\n",
    "            reddit_doc = RedditDocument(title=titre, auteur=auteur, date=date, url=url, texte=texte, num_comments=num_comments)\n",
    "            # doc_classe = Document(titre, auteur, date, url, texte)\n",
    "            \n",
    "            collection.append(reddit_doc)\n",
    "\n",
    "    # Création de l'index de documents\n",
    "    id2doc = {}\n",
    "    for i, doc in enumerate(collection):\n",
    "        id2doc[i] = doc.titre\n",
    "\n",
    "    # =============== 2.4, 2.5 : CLASSE AUTEURS ===============\n",
    "    from Classes import Author\n",
    "\n",
    "    # =============== 2.6 : DICT AUTEURS ===============\n",
    "    authors = {}\n",
    "    aut2id = {}\n",
    "    num_auteurs_vus = 0\n",
    "\n",
    "    # Création de la liste+index des Auteurs\n",
    "    for doc in collection:\n",
    "        if doc.auteur not in aut2id:\n",
    "            num_auteurs_vus += 1\n",
    "            authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "            aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "        authors[aut2id[doc.auteur]].add(doc.texte)\n",
    "\n",
    "\n",
    "    # =============== 2.7, 2.8 : CORPUS ===============\n",
    "    from Corpus import Corpus\n",
    "    corpus = Corpus(\"Mon corpus\")\n",
    "\n",
    "    # Construction du corpus à partir des documents\n",
    "    for doc in collection:\n",
    "        corpus.add(doc,)\n",
    "    #corpus.show(tri=\"abc\")\n",
    "    #print(repr(corpus))\n",
    "\n",
    "    # =============== 4.1 : TEST SINGLETON ===============\n",
    "    # corpus_test = Corpus(\"Mon corpus test\")\n",
    "    # print(\"Instance unique de corpus ?\", corpus is corpus_test)\n",
    "\n",
    "    \n",
    "    print(corpus.stats())\n",
    "    print(corpus.concordance(\"document\", context_size=10))\n",
    "    # =============== 2.9 : SAUVEGARDE ===============\n",
    "    import pickle\n",
    "\n",
    "    # Ouverture d'un fichier, puis écriture avec pickle\n",
    "    with open(\"corpus.pkl\", \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "    # Supression de la variable \"corpus\"\n",
    "    del corpus\n",
    "\n",
    "    # Ouverture du fichier, puis lecture avec pickle\n",
    "    with open(\"corpus.pkl\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    # La variable est réapparue\n",
    "    print(corpus)\n",
    "    \n",
    "\n",
    "if len(keyword) > 0:\n",
    "    button.on_click(rechercher_documents()) \n",
    "else:\n",
    "    print(\"Le mot clé est requis svp.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
