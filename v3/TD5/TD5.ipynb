{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db47f2abd154424c9739796ef930d192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='data', description='Mot cl√©:'), IntText(value=10, description='Nb articles')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "t = widgets.Text(\n",
    "    description='Mot cl√©:',\n",
    "    value='data',\n",
    "    disabled=False   \n",
    ")\n",
    "n = widgets.IntText( \n",
    "    description=\"Nb articles\",\n",
    "    value=10,\n",
    "    min=2\n",
    ")\n",
    "button = widgets.Button(\n",
    "    description='Valider',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Valider',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fonction de gestionnaire d'√©v√©nements pour le changement de valeur des champs\n",
    "def update_form(change):\n",
    "    if len(t.value.strip()) > 0 and n.value > 0:\n",
    "        # Afficher le message lorsque tous les champs sont remplis\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    else:\n",
    "        # Afficher le formulaire tant que tous les champs ne sont pas remplis\n",
    "        clear_output(wait=True)\n",
    "        display(form)\n",
    "\n",
    "# Liaison de la fonction de gestionnaire d'√©v√©nements aux changements de valeur des champs\n",
    "t.observe(update_form, names='value')\n",
    "n.observe(update_form, names='value')\n",
    "\n",
    "# Cr√©ation du formulaire\n",
    "form = widgets.VBox([t, n])\n",
    "\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit: 0 / 10\n",
      "ArXiv: 0 / 10\n",
      "# docs avec doublons : 20\n",
      "# docs sans doublons : 20\n",
      "Document 0\t# caract√®res : 0\t# mots : 1\t# phrases : 1\n",
      "Document 1\t# caract√®res : 635\t# mots : 102\t# phrases : 5\n",
      "Document 2\t# caract√®res : 803\t# mots : 128\t# phrases : 7\n",
      "Document 3\t# caract√®res : 315\t# mots : 52\t# phrases : 3\n",
      "Document 4\t# caract√®res : 166\t# mots : 29\t# phrases : 3\n",
      "Document 5\t# caract√®res : 1768\t# mots : 261\t# phrases : 18\n",
      "Document 6\t# caract√®res : 836\t# mots : 119\t# phrases : 8\n",
      "Document 7\t# caract√®res : 939\t# mots : 135\t# phrases : 9\n",
      "Document 8\t# caract√®res : 912\t# mots : 115\t# phrases : 7\n",
      "Document 9\t# caract√®res : 1139\t# mots : 161\t# phrases : 8\n",
      "Document 10\t# caract√®res : 270\t# mots : 49\t# phrases : 3\n",
      "Document 11\t# caract√®res : 1135\t# mots : 243\t# phrases : 9\n",
      "Document 12\t# caract√®res : 1238\t# mots : 182\t# phrases : 6\n",
      "Document 13\t# caract√®res : 294\t# mots : 46\t# phrases : 3\n",
      "Document 14\t# caract√®res : 988\t# mots : 134\t# phrases : 7\n",
      "Document 15\t# caract√®res : 399\t# mots : 72\t# phrases : 4\n",
      "Document 16\t# caract√®res : 1026\t# mots : 166\t# phrases : 7\n",
      "Document 17\t# caract√®res : 1234\t# mots : 161\t# phrases : 10\n",
      "Document 18\t# caract√®res : 745\t# mots : 109\t# phrases : 7\n",
      "3 workflow improvements we wish dbt announced at Coalesce 2023, par Pleasant-Guidance599\n",
      "Source: Reddit\n",
      "Number of Comments: 1\n",
      "A Random Sample Partition Data Model for Big Data Analysis, par Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara, Chenghao Wei, Heping He\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "A Survey on Sampling and Profiling over Big Data (Technical Report), par Zhicheng Liu, Aoqian Zhang\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking, par Zijian Ming, Chunjie Luo, Wanling Gao, Rui Han, Qiang Yang, Lei Wang, Jianfeng Zhan\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Coursera vs DataCamp, par Acceptable_Diet_1570\n",
      "Source: Reddit\n",
      "Number of Comments: 1\n",
      "Data Gathering from Path Constrained Mobile Sensors Using Data MULE, par Dinesh Dash\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Data Lakes, Clouds and Commons: A Review of Platforms for Analyzing and  Sharing Genomic Data, par Robert L. Grossman\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Data Science: A Comprehensive Overview, par Longbing Cao\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "How do I segregate a tv show script by the dialogues spoken by respective characters?, par alexturner_daddy\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "I shared a Python Pandas course (1.5 Hrs) on YouTube, par onurbaltaci\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "I'm looking for a way to find how \"happy\" the 100 most populated cities in America are, whether by depression rate, some kind of \"happiness index\", whatever. I don't know how/where to find this. I tried looking at CDC data websites but couldn't find anything by city. Can anyone help?, par SaladsAndSun\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "I'm moving countries, I got a desktop with important stuff on it, how do I back up my C drive (Everything) into a different hard drive?, par WaterEnderLunky\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Interview: How to find Duplicates from a table?, par lamhoangtriet\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Microservices based Linked Data Quality Model for Buildings Energy  Management Services, par Muhammad Aslam Jarwar, Sajjad Ali, Ilyoung Chong\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Modern Data Formats for Big Bioinformatics Data Analytics, par Shahzad Ahmed, M. Usman Ali, Javed Ferzund, Muhammad Atif Sarwar, Abbas Rehman, Atif Mehmood\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "Recovering past snapchats with .nomedia files, par WorldUpsideDowned\n",
      "Source: Reddit\n",
      "Number of Comments: 2\n",
      "Snapchat: \"delete is our default\" regarding data - That isn't true, is it?, par rnt206\n",
      "Source: Reddit\n",
      "Number of Comments: 4\n",
      "Technical Report: Developing a Working Data Hub, par Vijay Gadepally, Jeremy Kepner\n",
      "Source: Arxiv\n",
      "Co-authors: \n",
      "The journey to becoming a data maestro starts with the right tools. So, gear up, fellow data enthusiasts, and let Python pave the way to data analysis greatness! üêçüí°, par prasadnads92\n",
      "Source: Reddit\n",
      "Number of Comments: 0\n",
      "Toward a view-based data cleaning architecture, par Toshiyuki Shimizu, Hiroki Omori, Masatoshi Yoshikawa\n",
      "Source: Arxiv\n",
      "Co-authors: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============== PARTIE 1 =============\n",
    "# =============== 1.1 : REDDIT ===============\n",
    "# Libraries\n",
    "import praw\n",
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import datetime\n",
    "from Classes import ArxivDocument, RedditDocument\n",
    "\n",
    "limit = n.value\n",
    "keyword = t.value\n",
    "# Fonction affichage hi√©rarchie dict\n",
    "def showDictStruct(d):\n",
    "    def recursivePrint(d, i):\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                print(\"-\"*i, k)\n",
    "                recursivePrint(d[k], i+2)\n",
    "            else:\n",
    "                print(\"-\"*i, k, \":\", d[k])\n",
    "    recursivePrint(d, 1)\n",
    "\n",
    "def rechercher_documents():\n",
    "    \n",
    "    # Identification\n",
    "    reddit = praw.Reddit(client_id='QyizTPZdvFMfbmCtobzj1g', client_secret='TkvCDzi73SAaAge2LDOgEFHb6sF0NA', user_agent='test')\n",
    "\n",
    "    hot_posts = reddit.subreddit(keyword).hot(limit=limit)#.top(\"all\", limit=limit)#\n",
    "\n",
    "    # R√©cup√©ration du texte\n",
    "    docs = []\n",
    "    docs_bruts = []\n",
    "    afficher_cles = False\n",
    "    for i, post in enumerate(hot_posts):\n",
    "        if i%10==0: print(\"Reddit:\", i, \"/\", limit)\n",
    "        if afficher_cles:  # Pour conna√Ætre les diff√©rentes variables et leur contenu\n",
    "            for k, v in post.__dict__.items():\n",
    "                pass\n",
    "                print(k, \":\", v)\n",
    "\n",
    "        if post.selftext != \"\":  # Osef des posts sans texte\n",
    "            pass\n",
    "            #print(post.selftext)\n",
    "        docs.append(post.selftext.replace(\"\\n\", \" \"))\n",
    "        docs_bruts.append((\"Reddit\", post))\n",
    "\n",
    "    #print(docs)\n",
    "\n",
    "    # =============== 1.2 : ArXiv ===============\n",
    "\n",
    "    # Param√®tres\n",
    "    query_terms = [keyword]\n",
    "\n",
    "    # Requ√™te\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{\"+\".join(query_terms)}&start=0&max_results={limit}'\n",
    "    data = urllib.request.urlopen(url)\n",
    "\n",
    "    # Format dict (OrderedDict)\n",
    "    data = xmltodict.parse(data.read().decode('utf-8'))\n",
    "\n",
    "    #showDictStruct(data)\n",
    "\n",
    "    # Ajout r√©sum√©s √† la liste\n",
    "    for i, entry in enumerate(data[\"feed\"][\"entry\"]):\n",
    "        if i%10==0: print(\"ArXiv:\", i, \"/\", limit)\n",
    "        docs.append(entry[\"summary\"].replace(\"\\n\", \"\"))\n",
    "        docs_bruts.append((\"ArXiv\", entry))\n",
    "        #showDictStruct(entry)\n",
    "\n",
    "    # =============== 1.3 : Exploitation ===============\n",
    "    print(f\"# docs avec doublons : {len(docs)}\")\n",
    "    docs = list(set(docs))\n",
    "    print(f\"# docs sans doublons : {len(docs)}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i}\\t# caract√®res : {len(doc)}\\t# mots : {len(doc.split(' '))}\\t# phrases : {len(doc.split('.'))}\")\n",
    "        if len(doc)<100:\n",
    "            docs.remove(doc)\n",
    "\n",
    "    longueChaineDeCaracteres = \" \".join(docs)\n",
    "\n",
    "    # =============== PARTIE 2 =============\n",
    "\n",
    "    # =============== 2.3 : MANIPS ===============\n",
    "\n",
    "\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    for nature, doc in docs_bruts:\n",
    "        if nature == \"ArXiv\":  # Les fichiers de ArXiv ou de Reddit sont pas format√©s de la m√™me mani√®re √† ce stade.\n",
    "            #showDictStruct(doc)\n",
    "\n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enl√®ve les retours √† la ligne\n",
    "            try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, s√©par√©s par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enl√®ve les retours √† la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en ann√©e/mois/jour avec librairie datetime\n",
    "\n",
    "            # doc_classe = Document(titre, authors, date, doc[\"id\"], summary)  # Cr√©ation du Document\n",
    "            # Adding ArxivDocument\n",
    "            arxiv_doc = ArxivDocument(title=titre, auteur=authors, date=date, url=doc[\"id\"], texte=summary)\n",
    "\n",
    "            collection.append(arxiv_doc)  # Ajout du Document √† la liste.\n",
    "\n",
    "        elif nature == \"Reddit\":\n",
    "            #print(\"\".join([f\"{k}: {v}\\n\" for k, v in doc.__dict__.items()]))\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = \"https://www.reddit.com/\"+doc.permalink\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            num_comments = int(doc.num_comments)\n",
    "            # Adding RedditDocument\n",
    "            reddit_doc = RedditDocument(title=titre, auteur=auteur, date=date, url=url, texte=texte, num_comments=num_comments)\n",
    "            # doc_classe = Document(titre, auteur, date, url, texte)\n",
    "            \n",
    "            collection.append(reddit_doc)\n",
    "\n",
    "    # Cr√©ation de l'index de documents\n",
    "    id2doc = {}\n",
    "    for i, doc in enumerate(collection):\n",
    "        id2doc[i] = doc.titre\n",
    "\n",
    "    # =============== 2.4, 2.5 : CLASSE AUTEURS ===============\n",
    "    from Classes import Author\n",
    "\n",
    "    # =============== 2.6 : DICT AUTEURS ===============\n",
    "    authors = {}\n",
    "    aut2id = {}\n",
    "    num_auteurs_vus = 0\n",
    "\n",
    "    # Cr√©ation de la liste+index des Auteurs\n",
    "    for doc in collection:\n",
    "        if doc.auteur not in aut2id:\n",
    "            num_auteurs_vus += 1\n",
    "            authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "            aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "        authors[aut2id[doc.auteur]].add(doc.texte)\n",
    "\n",
    "\n",
    "    # =============== 2.7, 2.8 : CORPUS ===============\n",
    "    from Corpus import Corpus\n",
    "    corpus = Corpus(\"Mon corpus\")\n",
    "\n",
    "    # Construction du corpus √† partir des documents\n",
    "    for doc in collection:\n",
    "        corpus.add(doc,)\n",
    "    #corpus.show(tri=\"abc\")\n",
    "    #print(repr(corpus))\n",
    "\n",
    "    # =============== 4.1 : TEST SINGLETON ===============\n",
    "    # corpus_test = Corpus(\"Mon corpus test\")\n",
    "    # print(\"Instance unique de corpus ?\", corpus is corpus_test)\n",
    "\n",
    "    # =============== 2.9 : SAUVEGARDE ===============\n",
    "    import pickle\n",
    "\n",
    "    # Ouverture d'un fichier, puis √©criture avec pickle\n",
    "    with open(\"corpus.pkl\", \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "    # Supression de la variable \"corpus\"\n",
    "    del corpus\n",
    "\n",
    "    # Ouverture du fichier, puis lecture avec pickle\n",
    "    with open(\"corpus.pkl\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    # La variable est r√©apparue\n",
    "    print(corpus)\n",
    "\n",
    "if len(keyword) > 0:\n",
    "    button.on_click(rechercher_documents()) \n",
    "else:\n",
    "    print(\"Le mot cl√© est requis svp.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
